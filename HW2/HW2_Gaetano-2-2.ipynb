{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "63307197399648fa92368e5040a403f1",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 194162,
    "execution_start": 1671478881806,
    "source_hash": "fbbbf6fa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!unzip -nq /datasets/minispeechcommands/msc-train.zip\n",
    "#!unzip -nq /datasets/minispeechcommands/msc-val.zip\n",
    "#!unzip -nq /datasets/minispeechcommands/msc-test.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "9005ee15103d4932b3e5d0fa0e45ebe2",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5614,
    "execution_start": 1671478881807,
    "source_hash": "d5360d12",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-19 21:37:23.794892: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-19 21:37:23.864931: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-12-19 21:37:23.868174: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-19 21:37:23.868184: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-19 21:37:23.884826: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-19 21:37:24.430322: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 21:37:24.430364: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 21:37:24.430367: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow_io as tfio\n",
    "\n",
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6adbe3d463794fb892cb51b8e9e7f075",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "## Define Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "046f679528d74a06a86d4e14f9096370",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 45,
    "execution_start": 1671478887445,
    "source_hash": "786823ad",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-19 21:37:25.043528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-19 21:37:25.044072: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-19 21:37:25.044195: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-19 21:37:25.044278: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-19 21:37:25.044354: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-19 21:37:25.044450: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-19 21:37:25.044553: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-19 21:37:25.044630: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-19 21:37:25.044708: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-12-19 21:37:25.044723: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-12-19 21:37:25.046558: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "frame_length_in_s = 0.04\n",
    "\n",
    "PREPROCESSING_ARGS = {\n",
    "    'downsampling_rate': 16000,\n",
    "    'frame_length_in_s': frame_length_in_s,\n",
    "    'frame_step_in_s': frame_length_in_s,\n",
    "}\n",
    "\n",
    "TRAINING_ARGS = {\n",
    "    'batch_size': 32,\n",
    "    'initial_learning_rate': 0.015,\n",
    "    'end_learning_rate': 1.e-9,\n",
    "    'epochs': 60\n",
    "}\n",
    "\n",
    "\n",
    "final_sparsity = 0.5\n",
    "alpha=0.14\n",
    "\n",
    "num_mel_bins = (int) ((16000 - 16000 * PREPROCESSING_ARGS['frame_length_in_s'])/(16000*PREPROCESSING_ARGS['frame_step_in_s']))+1\n",
    "print(num_mel_bins)\n",
    "\n",
    "PREPROCESSING_ARGS = {\n",
    "    **PREPROCESSING_ARGS,\n",
    "    'num_mel_bins': num_mel_bins,\n",
    "    'lower_frequency': 20,\n",
    "    'upper_frequency': 4000,\n",
    "}\n",
    "\n",
    "LABELS = ['down', 'go', 'left', 'no', 'right', 'stop', 'up', 'yes']\n",
    "\n",
    "downsampling_rate = PREPROCESSING_ARGS['downsampling_rate']\n",
    "sampling_rate_int64 = tf.cast(downsampling_rate, tf.int64)\n",
    "frame_length = int(downsampling_rate * PREPROCESSING_ARGS['frame_length_in_s'])\n",
    "frame_step = int(downsampling_rate * PREPROCESSING_ARGS['frame_step_in_s'])\n",
    "num_spectrogram_bins = frame_length // 2 + 1\n",
    "num_mel_bins = PREPROCESSING_ARGS['num_mel_bins']\n",
    "lower_frequency = PREPROCESSING_ARGS['lower_frequency']\n",
    "upper_frequency = PREPROCESSING_ARGS['upper_frequency']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a80dec9cb5374651981bd099938b2954",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "## Create Train/Val/Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "edd270b402a740918a94988def04cb79",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 753,
    "execution_start": 1671478887488,
    "source_hash": "b531ec49",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.list_files(['msc-train/go*', 'msc-train/stop*'])\n",
    "val_ds = tf.data.Dataset.list_files(['msc-val/go*', 'msc-val/stop*'])\n",
    "test_ds = tf.data.Dataset.list_files(['msc-test/go*', 'msc-test/stop*'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "349f8aa6ebc44c5299ff4e3cd80f0fc7",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 68,
    "execution_start": 1671478888243,
    "source_hash": "a751ad73",
    "tags": []
   },
   "outputs": [],
   "source": [
    "linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
    "    num_mel_bins=num_mel_bins,\n",
    "    num_spectrogram_bins=num_spectrogram_bins,\n",
    "    sample_rate=downsampling_rate,\n",
    "    lower_edge_hertz=lower_frequency,\n",
    "    upper_edge_hertz=upper_frequency\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "96549602c2984e04b33398a4110a18a2",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 589,
    "execution_start": 1671478888354,
    "source_hash": "d56a80e8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(filename):\n",
    "    \n",
    "    audio_binary = tf.io.read_file(filename)\n",
    "\n",
    "    path_parts = tf.strings.split(filename, '/')\n",
    "    path_end = path_parts[-1]\n",
    "    file_parts = tf.strings.split(path_end, '_')\n",
    "    true_label = file_parts[0]\n",
    "    label_id = tf.argmax(true_label == LABELS)\n",
    "\n",
    "    audio, sampling_rate = tf.audio.decode_wav(audio_binary)\n",
    "    audio = tf.squeeze(audio)\n",
    "\n",
    "    zero_padding = tf.zeros(sampling_rate - tf.shape(audio), dtype=tf.float32)\n",
    "    audio_padded = tf.concat([audio, zero_padding], axis=0)\n",
    "\n",
    "    stft = tf.signal.stft(\n",
    "        audio_padded,\n",
    "        frame_length=frame_length,\n",
    "        frame_step=frame_step,\n",
    "        fft_length=frame_length\n",
    "    )\n",
    "    spectrogram = tf.abs(stft)\n",
    "    mel_spectrogram = tf.matmul(spectrogram, linear_to_mel_weight_matrix)\n",
    "    log_mel_spectrogram = tf.math.log(mel_spectrogram + 1.e-6)\n",
    "    log_mel_spectrogram = tf.expand_dims(log_mel_spectrogram, -1)  # channel axis\n",
    "    mfcss = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrogram)\n",
    "\n",
    "    return mfcss, label_id\n",
    "\n",
    "batch_size = TRAINING_ARGS['batch_size']\n",
    "epochs = TRAINING_ARGS['epochs']\n",
    "\n",
    "train_ds = train_ds.map(preprocess).batch(batch_size).cache()\n",
    "val_ds = val_ds.map(preprocess).batch(batch_size)\n",
    "test_ds = test_ds.map(preprocess).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "9f5b3e74633442c9a97b10e2650ad8f1",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 301,
    "execution_start": 1671478888944,
    "source_hash": "9dead3aa",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Shape: (32, 25, 25, 1)\n",
      "Data Shape: (25, 25, 1)\n",
      "Labels: tf.Tensor([1 5 1 5 5 1 5 1 5 1 5 5 1 5 5 1 1 5 1 1 5 1 5 5 5 1 5 1 1 5 5 5], shape=(32,), dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-19 21:37:25.835512: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for example_batch, example_labels in train_ds.take(1):\n",
    "  print('Batch Shape:', example_batch.shape)\n",
    "  print('Data Shape:', example_batch.shape[1:])\n",
    "  print('Labels:', example_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "8439d6e69c474bed9c24d8c489164197",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "## Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "4ae3a8231ec645a29a58215621cfa055",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 95,
    "execution_start": 1671478889259,
    "source_hash": "4e9a7cf8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=example_batch.shape[1:]),\n",
    "    tf.keras.layers.Conv2D(filters=int(128 * alpha), kernel_size=[3, 3], strides=[2, 2], use_bias=False, padding='valid'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.Conv2D(filters=int(128 * alpha), kernel_size=[3, 3], strides=[1, 1], use_bias=False, padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.Conv2D(filters=int(128 * alpha), kernel_size=[3, 3], strides=[1, 1], use_bias=False, padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(units=len(LABELS)),\n",
    "    tf.keras.layers.Softmax()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "bedd6bda120e45e59726072162078eab",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1017,
    "execution_start": 1671478889357,
    "source_hash": "8977dc7f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " prune_low_magnitude_conv2d   (None, 12, 12, 17)       308       \n",
      " (PruneLowMagnitude)                                             \n",
      "                                                                 \n",
      " prune_low_magnitude_batch_n  (None, 12, 12, 17)       69        \n",
      " ormalization (PruneLowMagni                                     \n",
      " tude)                                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_re_lu (  (None, 12, 12, 17)       1         \n",
      " PruneLowMagnitude)                                              \n",
      "                                                                 \n",
      " prune_low_magnitude_conv2d_  (None, 12, 12, 17)       5204      \n",
      " 1 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_batch_n  (None, 12, 12, 17)       69        \n",
      " ormalization_1 (PruneLowMag                                     \n",
      " nitude)                                                         \n",
      "                                                                 \n",
      " prune_low_magnitude_re_lu_1  (None, 12, 12, 17)       1         \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      " prune_low_magnitude_conv2d_  (None, 12, 12, 17)       5204      \n",
      " 2 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_batch_n  (None, 12, 12, 17)       69        \n",
      " ormalization_2 (PruneLowMag                                     \n",
      " nitude)                                                         \n",
      "                                                                 \n",
      " prune_low_magnitude_re_lu_2  (None, 12, 12, 17)       1         \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      " prune_low_magnitude_global_  (None, 17)               1         \n",
      " average_pooling2d (PruneLow                                     \n",
      " Magnitude)                                                      \n",
      "                                                                 \n",
      " prune_low_magnitude_dense (  (None, 8)                282       \n",
      " PruneLowMagnitude)                                              \n",
      "                                                                 \n",
      " prune_low_magnitude_softmax  (None, 8)                1         \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,210\n",
      "Trainable params: 5,601\n",
      "Non-trainable params: 5,609\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "begin_step = int(len(train_ds) * epochs * 0.2)\n",
    "end_step = int(len(train_ds) * epochs)\n",
    "\n",
    "pruning_params = {\n",
    "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "        initial_sparsity=0.40,\n",
    "        final_sparsity=final_sparsity,\n",
    "        begin_step=begin_step,\n",
    "        end_step=end_step\n",
    "    )\n",
    "}\n",
    "\n",
    "model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_id": "7d039a945b3e43c7bc1e56f4b92c1d45",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 194900,
    "execution_start": 1671478890203,
    "source_hash": "a751ad73",
    "tags": []
   },
   "outputs": [],
   "source": [
    "linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
    "    num_mel_bins=num_mel_bins,\n",
    "    num_spectrogram_bins=num_spectrogram_bins,\n",
    "    sample_rate=downsampling_rate,\n",
    "    lower_edge_hertz=lower_frequency,\n",
    "    upper_edge_hertz=upper_frequency\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6a63a47056984cb3be70ec93f721fabb",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "## Train the Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_id": "602d1458f847443baa5496c681fcee52",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 160247,
    "execution_start": 1671478890204,
    "source_hash": "1b2e08fb",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "50/50 [==============================] - 3s 23ms/step - loss: 0.7715 - sparse_categorical_accuracy: 0.6550 - val_loss: 1.2411 - val_sparse_categorical_accuracy: 0.6200\n",
      "Epoch 2/60\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.4042 - sparse_categorical_accuracy: 0.8281 - val_loss: 0.8201 - val_sparse_categorical_accuracy: 0.7900\n",
      "Epoch 3/60\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.3868 - sparse_categorical_accuracy: 0.8363 - val_loss: 1.6700 - val_sparse_categorical_accuracy: 0.6400\n",
      "Epoch 4/60\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.3343 - sparse_categorical_accuracy: 0.8612 - val_loss: 0.4783 - val_sparse_categorical_accuracy: 0.8300\n",
      "Epoch 5/60\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.2676 - sparse_categorical_accuracy: 0.8894 - val_loss: 0.3327 - val_sparse_categorical_accuracy: 0.8950\n",
      "Epoch 6/60\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.2227 - sparse_categorical_accuracy: 0.9137 - val_loss: 0.7260 - val_sparse_categorical_accuracy: 0.6950\n",
      "Epoch 7/60\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.1978 - sparse_categorical_accuracy: 0.9275 - val_loss: 0.7134 - val_sparse_categorical_accuracy: 0.7100\n",
      "Epoch 8/60\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.1755 - sparse_categorical_accuracy: 0.9294 - val_loss: 0.2556 - val_sparse_categorical_accuracy: 0.9200\n",
      "Epoch 9/60\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.1646 - sparse_categorical_accuracy: 0.9344 - val_loss: 0.3724 - val_sparse_categorical_accuracy: 0.8750\n",
      "Epoch 10/60\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.1535 - sparse_categorical_accuracy: 0.9369 - val_loss: 0.2658 - val_sparse_categorical_accuracy: 0.9150\n",
      "Epoch 11/60\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.1272 - sparse_categorical_accuracy: 0.9500 - val_loss: 0.2434 - val_sparse_categorical_accuracy: 0.9250\n",
      "Epoch 12/60\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.1113 - sparse_categorical_accuracy: 0.9563 - val_loss: 0.2462 - val_sparse_categorical_accuracy: 0.9250\n",
      "Epoch 13/60\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.1012 - sparse_categorical_accuracy: 0.9650 - val_loss: 0.2683 - val_sparse_categorical_accuracy: 0.9100\n",
      "Epoch 14/60\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0928 - sparse_categorical_accuracy: 0.9594 - val_loss: 0.4072 - val_sparse_categorical_accuracy: 0.8950\n",
      "Epoch 15/60\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0826 - sparse_categorical_accuracy: 0.9663 - val_loss: 0.3534 - val_sparse_categorical_accuracy: 0.9100\n",
      "Epoch 16/60\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0770 - sparse_categorical_accuracy: 0.9712 - val_loss: 0.3561 - val_sparse_categorical_accuracy: 0.9000\n",
      "Epoch 17/60\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0745 - sparse_categorical_accuracy: 0.9719 - val_loss: 0.3221 - val_sparse_categorical_accuracy: 0.9050\n",
      "Epoch 18/60\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0792 - sparse_categorical_accuracy: 0.9675 - val_loss: 0.3429 - val_sparse_categorical_accuracy: 0.9150\n",
      "Epoch 19/60\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0786 - sparse_categorical_accuracy: 0.9644 - val_loss: 0.4261 - val_sparse_categorical_accuracy: 0.9100\n",
      "Epoch 20/60\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0752 - sparse_categorical_accuracy: 0.9694 - val_loss: 0.3259 - val_sparse_categorical_accuracy: 0.9000\n",
      "Epoch 21/60\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0593 - sparse_categorical_accuracy: 0.9794 - val_loss: 0.2571 - val_sparse_categorical_accuracy: 0.9400\n",
      "Epoch 22/60\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0503 - sparse_categorical_accuracy: 0.9844 - val_loss: 0.3213 - val_sparse_categorical_accuracy: 0.8900\n",
      "Epoch 23/60\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0514 - sparse_categorical_accuracy: 0.9837 - val_loss: 0.2417 - val_sparse_categorical_accuracy: 0.9400\n",
      "Epoch 24/60\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0458 - sparse_categorical_accuracy: 0.9837 - val_loss: 0.2083 - val_sparse_categorical_accuracy: 0.9450\n",
      "Epoch 25/60\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0472 - sparse_categorical_accuracy: 0.9844 - val_loss: 0.2503 - val_sparse_categorical_accuracy: 0.9350\n",
      "Epoch 26/60\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0403 - sparse_categorical_accuracy: 0.9869 - val_loss: 0.2952 - val_sparse_categorical_accuracy: 0.9250\n",
      "Epoch 27/60\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0359 - sparse_categorical_accuracy: 0.9906 - val_loss: 0.3250 - val_sparse_categorical_accuracy: 0.9200\n",
      "Epoch 28/60\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0342 - sparse_categorical_accuracy: 0.9894 - val_loss: 0.3445 - val_sparse_categorical_accuracy: 0.9100\n",
      "Epoch 29/60\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0341 - sparse_categorical_accuracy: 0.9906 - val_loss: 0.3717 - val_sparse_categorical_accuracy: 0.9100\n",
      "Epoch 30/60\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0285 - sparse_categorical_accuracy: 0.9919 - val_loss: 0.2103 - val_sparse_categorical_accuracy: 0.9550\n",
      "Epoch 31/60\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0402 - sparse_categorical_accuracy: 0.9875 - val_loss: 0.3813 - val_sparse_categorical_accuracy: 0.8800\n",
      "Epoch 32/60\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0398 - sparse_categorical_accuracy: 0.9831 - val_loss: 0.2135 - val_sparse_categorical_accuracy: 0.9400\n",
      "Epoch 33/60\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0279 - sparse_categorical_accuracy: 0.9912 - val_loss: 0.2681 - val_sparse_categorical_accuracy: 0.9350\n",
      "Epoch 34/60\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0262 - sparse_categorical_accuracy: 0.9925 - val_loss: 0.4266 - val_sparse_categorical_accuracy: 0.9000\n",
      "Epoch 35/60\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0234 - sparse_categorical_accuracy: 0.9944 - val_loss: 0.4820 - val_sparse_categorical_accuracy: 0.8750\n",
      "Epoch 36/60\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0223 - sparse_categorical_accuracy: 0.9937 - val_loss: 0.3990 - val_sparse_categorical_accuracy: 0.8850\n",
      "Epoch 37/60\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0217 - sparse_categorical_accuracy: 0.9944 - val_loss: 0.3518 - val_sparse_categorical_accuracy: 0.9150\n",
      "Epoch 38/60\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0212 - sparse_categorical_accuracy: 0.9937 - val_loss: 0.3272 - val_sparse_categorical_accuracy: 0.9300\n",
      "Epoch 39/60\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0198 - sparse_categorical_accuracy: 0.9937 - val_loss: 0.3117 - val_sparse_categorical_accuracy: 0.9400\n",
      "Epoch 40/60\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0191 - sparse_categorical_accuracy: 0.9956 - val_loss: 0.2705 - val_sparse_categorical_accuracy: 0.9400\n",
      "Epoch 41/60\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0185 - sparse_categorical_accuracy: 0.9956 - val_loss: 0.2649 - val_sparse_categorical_accuracy: 0.9550\n",
      "Epoch 42/60\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0181 - sparse_categorical_accuracy: 0.9956 - val_loss: 0.2695 - val_sparse_categorical_accuracy: 0.9450\n",
      "Epoch 43/60\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0181 - sparse_categorical_accuracy: 0.9950 - val_loss: 0.2596 - val_sparse_categorical_accuracy: 0.9500\n",
      "Epoch 44/60\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0179 - sparse_categorical_accuracy: 0.9944 - val_loss: 0.2446 - val_sparse_categorical_accuracy: 0.9550\n",
      "Epoch 45/60\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0177 - sparse_categorical_accuracy: 0.9956 - val_loss: 0.2481 - val_sparse_categorical_accuracy: 0.9500\n",
      "Epoch 46/60\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0169 - sparse_categorical_accuracy: 0.9950 - val_loss: 0.2508 - val_sparse_categorical_accuracy: 0.9550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/60\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0158 - sparse_categorical_accuracy: 0.9956 - val_loss: 0.2566 - val_sparse_categorical_accuracy: 0.9550\n",
      "Epoch 48/60\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0142 - sparse_categorical_accuracy: 0.9962 - val_loss: 0.2624 - val_sparse_categorical_accuracy: 0.9500\n",
      "Epoch 49/60\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0132 - sparse_categorical_accuracy: 0.9962 - val_loss: 0.2653 - val_sparse_categorical_accuracy: 0.9500\n",
      "Epoch 50/60\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0126 - sparse_categorical_accuracy: 0.9969 - val_loss: 0.2610 - val_sparse_categorical_accuracy: 0.9500\n",
      "Epoch 51/60\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0124 - sparse_categorical_accuracy: 0.9969 - val_loss: 0.2548 - val_sparse_categorical_accuracy: 0.9550\n",
      "Epoch 52/60\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0124 - sparse_categorical_accuracy: 0.9969 - val_loss: 0.2458 - val_sparse_categorical_accuracy: 0.9550\n",
      "Epoch 53/60\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0128 - sparse_categorical_accuracy: 0.9975 - val_loss: 0.2361 - val_sparse_categorical_accuracy: 0.9600\n",
      "Epoch 54/60\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0134 - sparse_categorical_accuracy: 0.9950 - val_loss: 0.2341 - val_sparse_categorical_accuracy: 0.9500\n",
      "Epoch 55/60\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0138 - sparse_categorical_accuracy: 0.9956 - val_loss: 0.2543 - val_sparse_categorical_accuracy: 0.9350\n",
      "Epoch 56/60\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0137 - sparse_categorical_accuracy: 0.9969 - val_loss: 0.2599 - val_sparse_categorical_accuracy: 0.9350\n",
      "Epoch 57/60\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0127 - sparse_categorical_accuracy: 0.9969 - val_loss: 0.2373 - val_sparse_categorical_accuracy: 0.9400\n",
      "Epoch 58/60\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0116 - sparse_categorical_accuracy: 0.9969 - val_loss: 0.2291 - val_sparse_categorical_accuracy: 0.9600\n",
      "Epoch 59/60\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0109 - sparse_categorical_accuracy: 0.9969 - val_loss: 0.2303 - val_sparse_categorical_accuracy: 0.9600\n",
      "Epoch 60/60\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0104 - sparse_categorical_accuracy: 0.9969 - val_loss: 0.2212 - val_sparse_categorical_accuracy: 0.9500\n"
     ]
    }
   ],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "initial_learning_rate = TRAINING_ARGS['initial_learning_rate']\n",
    "end_learning_rate = TRAINING_ARGS['end_learning_rate']\n",
    "\n",
    "linear_decay = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    end_learning_rate=end_learning_rate,\n",
    "    decay_steps=len(train_ds) * epochs,\n",
    ")\n",
    "optimizer = tf.optimizers.Adam(learning_rate=linear_decay)\n",
    "metrics = [tf.metrics.SparseCategoricalAccuracy()]\n",
    "callbacks = [tfmot.sparsity.keras.UpdatePruningStep()]\n",
    "model_for_pruning.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "history = model_for_pruning.fit(train_ds, epochs=epochs, validation_data=val_ds,callbacks=callbacks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_id": "133e76730d2f4023b566863ac354e5d5",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 1204,
    "execution_start": 1671478855553,
    "source_hash": "a1400dd8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0938 - sparse_categorical_accuracy: 0.9750\n",
      "Training Loss: 0.0104\n",
      "Training Accuracy: 99.69%\n",
      "\n",
      "Validation Loss: 0.2212\n",
      "Validation Accuracy: 95.00%\n",
      "\n",
      "Test Loss: 0.0938\n",
      "Test Accuracy: 97.50%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model_for_pruning.evaluate(test_ds)\n",
    "\n",
    "training_loss = history.history['loss'][-1]\n",
    "training_accuracy = history.history['sparse_categorical_accuracy'][-1]\n",
    "val_loss = history.history['val_loss'][-1]\n",
    "val_accuracy = history.history['val_sparse_categorical_accuracy'][-1]\n",
    "\n",
    "print(f'Training Loss: {training_loss:.4f}')\n",
    "print(f'Training Accuracy: {training_accuracy*100.:.2f}%')\n",
    "print()\n",
    "print(f'Validation Loss: {val_loss:.4f}')\n",
    "print(f'Validation Accuracy: {val_accuracy*100.:.2f}%')\n",
    "print()\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy*100.:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "679034c14b9841f2811ae96d29d044b8",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "## TF model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_id": "4cd49885022842c89560a1cb092575f8",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 1068,
    "execution_start": 1671478856754,
    "source_hash": "3ae18422",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_models/model13/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_models/model13/assets\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "timestamp = int(time())\n",
    "modelName = 'model13'\n",
    "\n",
    "saved_model_dir = f'./saved_models/{modelName}'\n",
    "if not os.path.exists(saved_model_dir):\n",
    "    os.makedirs(saved_model_dir)\n",
    "model.save(saved_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "876281c1396b4036b6fb90216ddcf6b9",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "## TF convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cell_id": "664f9c0137c44c64ac34cf749e56806e",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 441,
    "execution_start": 1671478857826,
    "source_hash": "c6196e07",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original TFLite Size (pruned model): 24.73 KB\n",
      "ZIP TFLite Size (pruned model): 14.10 KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-19 21:37:57.414014: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2022-12-19 21:37:57.414033: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2022-12-19 21:37:57.414421: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: ./saved_models/model13\n",
      "2022-12-19 21:37:57.416020: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2022-12-19 21:37:57.416041: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: ./saved_models/model13\n",
      "2022-12-19 21:37:57.420191: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "2022-12-19 21:37:57.421305: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2022-12-19 21:37:57.449289: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: ./saved_models/model13\n",
      "2022-12-19 21:37:57.457453: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 43033 microseconds.\n",
      "2022-12-19 21:37:57.474595: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = modelName\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(f'./saved_models/{MODEL_NAME}')\n",
    "tflite_model = converter.convert()\n",
    "tflite_models_dir = './tflite_models'\n",
    "if not os.path.exists(tflite_models_dir):\n",
    "    os.makedirs(tflite_models_dir)\n",
    "tflite_model_name = os.path.join(tflite_models_dir, f'{MODEL_NAME}.tflite')\n",
    "tflite_model_name\n",
    "with open(tflite_model_name, 'wb') as fp:\n",
    "    fp.write(tflite_model)\n",
    "\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(f'{tflite_model_name}.zip', 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "    f.write(tflite_model_name)\n",
    "\n",
    "pruned_tflite_size = os.path.getsize(tflite_model_name) / 1024\n",
    "pruned_zip_size = os.path.getsize(f'{tflite_model_name}.zip') / 1024\n",
    "\n",
    "print(f'Original TFLite Size (pruned model): {pruned_tflite_size:.2f} KB')\n",
    "print(f'ZIP TFLite Size (pruned model): {pruned_zip_size:.2f} KB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cell_id": "d0fb88ca647e4325897c8a164ac286e9",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 4,
    "execution_start": 1671478858270,
    "source_hash": "4af54417",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of inputs: 1\n",
      "Input name: serving_default_input_1:0\n",
      "Input index: 0\n",
      "Number of output: 1\n",
      "Output name: StatefulPartitionedCall:0\n",
      "Output index: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=f'./tflite_models/{MODEL_NAME}.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "print('Number of inputs:', len(input_details))\n",
    "print('Input name:', input_details[0]['name'])\n",
    "print('Input index:', input_details[0]['index'])\n",
    "print('Number of output:', len(output_details))\n",
    "print('Output name:', output_details[0]['name'])\n",
    "print('Output index:', output_details[0]['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cell_id": "986377244c23488a9ce78f071bd5cf21",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 5,
    "execution_start": 1671478858273,
    "source_hash": "6886c60b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "filenames = glob('msc-test/go*') + glob('msc-test/stop*')\n",
    "print(len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cell_id": "c890f633138f42c18f590b9b548ec1e3",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 2500,
    "execution_start": 1671478858282,
    "source_hash": "ca5de2c1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "\n",
    "avg_preprocessing_latency = 0\n",
    "avg_model_latency = 0\n",
    "latencies = []\n",
    "accuracy = 0.0\n",
    "\n",
    "for filename in filenames:\n",
    "    audio_binary = tf.io.read_file(filename)\n",
    "\n",
    "    # NEED ONLY FOR TESTING\n",
    "    path_parts = tf.strings.split(filename, '/')\n",
    "    path_end = path_parts[-1]\n",
    "    file_parts = tf.strings.split(path_end, '_')\n",
    "    true_label = file_parts[0]\n",
    "    true_label = true_label.numpy().decode()\n",
    "\n",
    "    # PRE-PROCESSING (LOG-MEL SPECTROGRAM)\n",
    "    start_preprocess = time()\n",
    "    audio, sampling_rate = tf.audio.decode_wav(audio_binary)\n",
    "    audio = tf.squeeze(audio)\n",
    "\n",
    "    zero_padding = tf.zeros(sampling_rate - tf.shape(audio), dtype=tf.float32)\n",
    "    audio_padded = tf.concat([audio, zero_padding], axis=0)\n",
    "\n",
    "    if downsampling_rate != sampling_rate:\n",
    "        audio_padded = tfio.audio.resample(audio_padded, sampling_rate_int64, downsampling_rate)\n",
    "\n",
    "    stft = tf.signal.stft(\n",
    "        audio_padded,\n",
    "        frame_length=frame_length,\n",
    "        frame_step=frame_step,\n",
    "        fft_length=frame_length\n",
    "    )\n",
    "    spectrogram = tf.abs(stft)\n",
    "    mel_spectrogram = tf.matmul(spectrogram, linear_to_mel_weight_matrix)\n",
    "    log_mel_spectrogram = tf.math.log(mel_spectrogram + 1.e-6)\n",
    "    log_mel_spectrogram = tf.expand_dims(log_mel_spectrogram, 0)  # batch axis\n",
    "    log_mel_spectrogram = tf.expand_dims(log_mel_spectrogram, -1)  # channel axis\n",
    "    mfcss = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrogram)\n",
    "    end_preprocess = time()\n",
    "    interpreter.set_tensor(input_details[0]['index'], mfcss)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[0]['index'])\n",
    "    end_inference = time()\n",
    "\n",
    "    top_index = np.argmax(output[0])\n",
    "    predicted_label = LABELS[top_index]\n",
    "\n",
    "    accuracy += true_label == predicted_label\n",
    "    avg_preprocessing_latency += (end_preprocess - start_preprocess)\n",
    "    avg_model_latency += (end_inference - end_preprocess)\n",
    "    latencies.append(end_inference - start_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cell_id": "740390403616433bb7b03d07057c5625",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 4,
    "execution_start": 1671478860786,
    "source_hash": "4a5c9a9d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.500%\n",
      "Model size: 24.7 KB\n",
      "Preprocessing Latency: 5.1 ms\n",
      "Model Latency: 0.1 ms\n",
      "Total Latency: 4.9 ms\n"
     ]
    }
   ],
   "source": [
    "accuracy /= len(filenames)\n",
    "avg_preprocessing_latency /= len(filenames)\n",
    "avg_model_latency /= len(filenames)\n",
    "total_latency = np.median(latencies)\n",
    "\n",
    "import os\n",
    "\n",
    "model_size = os.path.getsize(f'./tflite_models/{MODEL_NAME}.tflite')\n",
    "\n",
    "print(f'Accuracy: {accuracy * 100.:.3f}%')\n",
    "print(f'Model size: {model_size / 1024:.1f} KB')\n",
    "print(f'Preprocessing Latency: {1000 * avg_preprocessing_latency:.1f} ms')\n",
    "print(f'Model Latency: {1000 * avg_model_latency:.1f} ms')\n",
    "print(f'Total Latency: {1000 * total_latency:.1f} ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=1a21d5cf-232d-49da-b53d-f1e5025e08e5' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "a0e620be69c447808ebb8eda6f889343",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
